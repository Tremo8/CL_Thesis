{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install avalanche-lib==0.3.1\n",
    "! pip install micromind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.training.supervised import Naive\n",
    "from avalanche.training import JointTraining, Cumulative\n",
    "from avalanche.training.plugins import EvaluationPlugin, ReplayPlugin\n",
    "from avalanche.evaluation.metrics import accuracy_metrics, loss_metrics\n",
    "from avalanche.logging import InteractiveLogger\n",
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from avalanche.models import SimpleMLP # Import Multi-Layer Perceptron with custom parameters\n",
    "import torch.nn as nn\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_mnist = SplitMNIST(n_experiences=5, seed=1222, return_task_id = True)\n",
    "\n",
    "# recovering the train and test streams\n",
    "train_stream = split_mnist.train_stream\n",
    "test_stream = split_mnist.test_stream"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train(model, optimizer, criterion, train_loader):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels, _ in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        prob = nn.functional.softmax(outputs, dim=1)\n",
    "        _, predicted = prob.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    average_loss = train_loss / len(train_loader)\n",
    "\n",
    "    return accuracy, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, criterion, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            prob = nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted = prob.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    average_loss = test_loss / len(test_loader)\n",
    "\n",
    "    return accuracy, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "def concat_experience(train_stream, test_stream):\n",
    "    length = len(train_stream)\n",
    "    for i in range(0,length):\n",
    "        if i == 0:\n",
    "            train_loader = train_stream[i].dataset\n",
    "            test_loader = test_stream[i].dataset\n",
    "        else:\n",
    "            train_loader = torch.utils.data.ConcatDataset([train_loader, train_stream[i].dataset])\n",
    "            test_loader = torch.utils.data.ConcatDataset([test_loader, test_stream[i].dataset])\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micromind import PhiNet\n",
    "MLP1_SGD = SimpleMLP(num_classes=10)\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "optimizer = SGD(MLP1_SGD.parameters(), lr=learning_rate, momentum=momentum)\n",
    "optimizer = Adam(MLP1_SGD.parameters(), lr=0.01, weight_decay=0.01)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "model = PhiNet(input_shape = (1, 28, 28), alpha = 0.5, beta = 1, t_zero = 6,num_layers=4 ,include_top = True, num_classes = 10).to(device)\n",
    "Adam_opt = Adam(model.parameters(), lr=0.01, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = concat_experience(train_stream, test_stream)\n",
    "print(len(train_loader))\n",
    "train_loader = torch.utils.data.DataLoader(train_loader, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_loader, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_acc, train_loss = utils.train(model, optimizer, criterion, train_loader)\n",
    "    test_acc, test_loss = utils.test(model, criterion, test_loader)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experience in split_mnist.test_stream:\n",
    "  print(\"Start of task \", experience.task_label)\n",
    "  print('Classes in this task:', experience.classes_in_this_experience)\n",
    "  experience_dataloader = torch.utils.data.DataLoader(experience.dataset, batch_size=64, shuffle=False)\n",
    "  test_acc, test_loss = test(model, criterion, experience_dataloader)\n",
    "  print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
