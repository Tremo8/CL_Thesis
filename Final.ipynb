{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwj4CkTzzcWr"
      },
      "source": [
        "# Latent Replay for Continual Learning on Edge devices with Efficient Architectures\n",
        "In this Jupyter Notebook, we will explore various deep learning strategies for continual learning using the PyTorch framework. We will use the Avalanche library to handle the continual learning benchmarks, and we will train and evaluate different models and strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPXJaZZjzcWu"
      },
      "source": [
        "## Installation\n",
        "Install all the necessary library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzF55R6dzcWv",
        "outputId": "d65bed89-f433-46e7-b780-98c68180e51c"
      },
      "outputs": [],
      "source": [
        "! pip install avalanche-lib==0.3.1\n",
        "! pip install micromind\n",
        "! pip install torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing the necessary libraries and setting up the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E3yHNRPzcWw",
        "outputId": "b9bbc510-11f7-4381-d4dc-5cfb5c67f2a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\matte\\anaconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x20f7f029fb0>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Standard libraries\n",
        "import json\n",
        "\n",
        "# PyTorch modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import SGD, Adam\n",
        "\n",
        "# Torchvision modules\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Avalanche modules\n",
        "from avalanche.benchmarks.classic import SplitMNIST, SplitCIFAR10, OpenLORIS\n",
        "from avalanche.models import MobilenetV1\n",
        "from avalanche.training.storage_policy import ReservoirSamplingBuffer\n",
        "\n",
        "# Custom Strategy modules\n",
        "from strategy.joint_training import JointTraining\n",
        "from strategy.fine_tuning import FineTuning\n",
        "from strategy.comulative import Comulative\n",
        "from strategy.replay import Replay\n",
        "from strategy.latent_replay import LatentReplay\n",
        "\n",
        "# Model modules\n",
        "from micromind import PhiNet\n",
        "from model.phinet_v2 import PhiNet_v2\n",
        "from model.phinet_v3 import PhiNetV3\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "# Uutils modules\n",
        "import utility.utils as utils\n",
        "import utility.evaluations as evaluations\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KDShngxzcWy"
      },
      "source": [
        "## Benchmark\n",
        "In this section, we will prepare the data for the continual learning experiments. We will use two classic benchmark datasets: \n",
        "- SplitMNIST;\n",
        "- SplitCIFAR10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MNIST\n",
        "The SplitMNIST dataset with 5 experiences. The SplitMNIST is normailize in Avalnche with `mean = 0.1307` and `std = 0.3081`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i52l8jKzcWy",
        "outputId": "668663c0-52d7-4877-a8d0-d8de1fc5a070"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to C:\\Users\\matte\\.avalanche\\data\\mnist\\TensorMNIST\\raw\\train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 89%|████████▉ | 8814592/9912422 [01:00<00:07, 145162.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "ConnectionResetError",
          "evalue": "[WinError 10054] An existing connection was forcibly closed by the remote host",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m split_mnist \u001b[39m=\u001b[39m SplitMNIST(n_experiences\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, seed\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, return_task_id \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      3\u001b[0m \u001b[39m# recovering the train and test streams\u001b[39;00m\n\u001b[0;32m      4\u001b[0m train_stream \u001b[39m=\u001b[39m split_mnist\u001b[39m.\u001b[39mtrain_stream\n",
            "File \u001b[1;32mc:\\Users\\matte\\anaconda3\\lib\\site-packages\\avalanche\\benchmarks\\classic\\cmnist.py:145\u001b[0m, in \u001b[0;36mSplitMNIST\u001b[1;34m(n_experiences, return_task_id, seed, fixed_class_order, shuffle, class_ids_from_zero_in_each_exp, train_transform, eval_transform, dataset_root)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mSplitMNIST\u001b[39m(\n\u001b[0;32m     73\u001b[0m     n_experiences: \u001b[39mint\u001b[39m,\n\u001b[0;32m     74\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m     dataset_root: Union[\u001b[39mstr\u001b[39m, Path] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     83\u001b[0m ):\n\u001b[0;32m     84\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39m    Creates a CL benchmark using the MNIST dataset.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39m    :returns: A properly initialized :class:`NCScenario` instance.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m     mnist_train, mnist_test \u001b[39m=\u001b[39m get_mnist_dataset(dataset_root)\n\u001b[0;32m    147\u001b[0m     \u001b[39mreturn\u001b[39;00m nc_benchmark(\n\u001b[0;32m    148\u001b[0m         train_dataset\u001b[39m=\u001b[39mmnist_train,\n\u001b[0;32m    149\u001b[0m         test_dataset\u001b[39m=\u001b[39mmnist_test,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m         eval_transform\u001b[39m=\u001b[39meval_transform,\n\u001b[0;32m    158\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\matte\\anaconda3\\lib\\site-packages\\avalanche\\benchmarks\\datasets\\external_datasets\\mnist.py:31\u001b[0m, in \u001b[0;36mget_mnist_dataset\u001b[1;34m(dataset_root)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mif\u001b[39;00m dataset_root \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     dataset_root \u001b[39m=\u001b[39m default_dataset_location(\u001b[39m\"\u001b[39m\u001b[39mmnist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m train_set \u001b[39m=\u001b[39m TensorMNIST(root\u001b[39m=\u001b[39;49mdataset_root, train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, download\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     33\u001b[0m test_set \u001b[39m=\u001b[39m TensorMNIST(root\u001b[39m=\u001b[39mdataset_root, train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     35\u001b[0m \u001b[39mreturn\u001b[39;00m train_set, test_set\n",
            "File \u001b[1;32mc:\\Users\\matte\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:99\u001b[0m, in \u001b[0;36mMNIST.__init__\u001b[1;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m download:\n\u001b[1;32m---> 99\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownload()\n\u001b[0;32m    101\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_exists():\n\u001b[0;32m    102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDataset not found. You can use download=True to download it\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\matte\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:187\u001b[0m, in \u001b[0;36mMNIST.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 187\u001b[0m     download_and_extract_archive(url, download_root\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_folder, filename\u001b[39m=\u001b[39;49mfilename, md5\u001b[39m=\u001b[39;49mmd5)\n\u001b[0;32m    188\u001b[0m \u001b[39mexcept\u001b[39;00m URLError \u001b[39mas\u001b[39;00m error:\n\u001b[0;32m    189\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to download (trying next):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00merror\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\matte\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\utils.py:434\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[1;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filename:\n\u001b[0;32m    432\u001b[0m     filename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(url)\n\u001b[1;32m--> 434\u001b[0m download_url(url, download_root, filename, md5)\n\u001b[0;32m    436\u001b[0m archive \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(download_root, filename)\n\u001b[0;32m    437\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExtracting \u001b[39m\u001b[39m{\u001b[39;00marchive\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00mextract_root\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\matte\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\utils.py:151\u001b[0m, in \u001b[0;36mdownload_url\u001b[1;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[0;32m    149\u001b[0m             _urlretrieve(url, fpath)\n\u001b[0;32m    150\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    153\u001b[0m \u001b[39m# check integrity of downloaded file\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m check_integrity(fpath, md5):\n",
            "File \u001b[1;32mc:\\Users\\matte\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\utils.py:144\u001b[0m, in \u001b[0;36mdownload_url\u001b[1;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDownloading \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m url \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m to \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m fpath)\n\u001b[1;32m--> 144\u001b[0m     _urlretrieve(url, fpath)\n\u001b[0;32m    145\u001b[0m \u001b[39mexcept\u001b[39;00m (urllib\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mURLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[39mif\u001b[39;00m url[:\u001b[39m5\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps\u001b[39m\u001b[39m\"\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\matte\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\utils.py:48\u001b[0m, in \u001b[0;36m_urlretrieve\u001b[1;34m(url, filename, chunk_size)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_urlretrieve\u001b[39m(url: \u001b[39mstr\u001b[39m, filename: \u001b[39mstr\u001b[39m, chunk_size: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m32\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[39mwith\u001b[39;00m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39murlopen(urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39mRequest(url, headers\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m: USER_AGENT})) \u001b[39mas\u001b[39;00m response:\n\u001b[1;32m---> 48\u001b[0m         _save_response_content(\u001b[39miter\u001b[39;49m(\u001b[39mlambda\u001b[39;49;00m: response\u001b[39m.\u001b[39;49mread(chunk_size), \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m), filename, length\u001b[39m=\u001b[39;49mresponse\u001b[39m.\u001b[39;49mlength)\n",
            "File \u001b[1;32mc:\\Users\\matte\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\utils.py:37\u001b[0m, in \u001b[0;36m_save_response_content\u001b[1;34m(content, destination, length)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_save_response_content\u001b[39m(\n\u001b[0;32m     32\u001b[0m     content: Iterator[\u001b[39mbytes\u001b[39m],\n\u001b[0;32m     33\u001b[0m     destination: \u001b[39mstr\u001b[39m,\n\u001b[0;32m     34\u001b[0m     length: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     35\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(destination, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m fh, tqdm(total\u001b[39m=\u001b[39mlength) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m---> 37\u001b[0m         \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m content:\n\u001b[0;32m     38\u001b[0m             \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m     39\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunk:\n\u001b[0;32m     40\u001b[0m                 \u001b[39mcontinue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\matte\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\utils.py:48\u001b[0m, in \u001b[0;36m_urlretrieve.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_urlretrieve\u001b[39m(url: \u001b[39mstr\u001b[39m, filename: \u001b[39mstr\u001b[39m, chunk_size: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m32\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[39mwith\u001b[39;00m urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39murlopen(urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39mRequest(url, headers\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m: USER_AGENT})) \u001b[39mas\u001b[39;00m response:\n\u001b[1;32m---> 48\u001b[0m         _save_response_content(\u001b[39miter\u001b[39m(\u001b[39mlambda\u001b[39;00m: response\u001b[39m.\u001b[39;49mread(chunk_size), \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m), filename, length\u001b[39m=\u001b[39mresponse\u001b[39m.\u001b[39mlength)\n",
            "File \u001b[1;32mc:\\Users\\matte\\anaconda3\\lib\\http\\client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    463\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 465\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    467\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
            "File \u001b[1;32mc:\\Users\\matte\\anaconda3\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] An existing connection was forcibly closed by the remote host"
          ]
        }
      ],
      "source": [
        "split_mnist = SplitMNIST(n_experiences=5, seed=0, return_task_id = True)\n",
        "\n",
        "# recovering the train and test streams\n",
        "train_stream = split_mnist.train_stream\n",
        "test_stream = split_mnist.test_stream\n",
        "\n",
        "input_size = train_stream[0].dataset[0][0].shape\n",
        "\n",
        "lr = 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CIFAR10\n",
        "The SplitCIFAR10 dataset with 5 experiences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Resize((160, 160)),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "split_cifar = SplitCIFAR10(n_experiences=5, seed=0, return_task_id = True, train_transform = transform, eval_transform = transform)\n",
        "\n",
        "# recovering the train and test streams\n",
        "train_stream = split_cifar.train_stream\n",
        "test_stream = split_cifar.test_stream\n",
        "\n",
        "input_size = train_stream[0].dataset[0][0].shape\n",
        "\n",
        "lr = 0.005"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OpenLORIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "openloris = OpenLORIS()\n",
        "\n",
        "# recovering the train and test streams\n",
        "train_stream = split_cifar.train_stream\n",
        "test_stream = split_cifar.test_stream\n",
        "\n",
        "print(\"Train stream size: \", len(train_stream))\n",
        "print(\"Test stream size: \", len(test_stream))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GApUVD8lzcWz"
      },
      "source": [
        "## Training\n",
        "Set up the necessary components for training our deep learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvfBVXfazcW1"
      },
      "outputs": [],
      "source": [
        "# Loss criterion\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training and evaluation batch size parameters\n",
        "train_mb_size = 128\n",
        "eval_mb_size = 128\n",
        "\n",
        "# Train-Validation split ratio\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Early stopping patience\n",
        "patience = 3\n",
        "\n",
        "# Accuracies dictionary to store accuracies for each strategy\n",
        "accs = dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWJxGNh9zcW2"
      },
      "source": [
        "### Fine Tuning Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "vJFWY3eVzcW2",
        "outputId": "4b52800b-279b-4dfd-bd63-31ec216d6370"
      },
      "outputs": [],
      "source": [
        "#model1 = PhiNet(input_shape = (1, 28, 28), alpha = 0.5, beta = 1, t_zero = 6,num_layers=7 ,include_top = True, num_classes = 10).to(device)\n",
        "model1 = PhiNet.from_pretrained(\"CIFAR-10\", 3.0, 0.75, 6.0, 7, 160, classifier=True).to(device)\n",
        "optimizer1 = Adam(model1.parameters(), lr=lr, weight_decay=0)\n",
        "\n",
        "train_epochs = 10\n",
        "\n",
        "fine_tuning = FineTuning(\n",
        "    model=model1,\n",
        "    optimizer=optimizer1,\n",
        "    criterion=criterion,\n",
        "    train_mb_size=train_mb_size,\n",
        "    train_epochs=train_epochs,\n",
        "    eval_mb_size=eval_mb_size,\n",
        "    split_ratio = split_ratio,\n",
        "    patience = patience,\n",
        "    device=device,\n",
        "    path = 'results/models/fine_tuning.pth'\n",
        ")\n",
        "\n",
        "fine_tuning.train(train_stream, test_stream, plotting=True)\n",
        "b,c = fine_tuning.test(test_stream)\n",
        "a = fine_tuning.get_tasks_acc()\n",
        "accs['Fine Tuning'] = a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-tRVDugzcW3"
      },
      "source": [
        "### Joint Training Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsNBEspqzcW4"
      },
      "outputs": [],
      "source": [
        "#model2 = PhiNet(input_shape = (1, 28, 28), alpha = 0.5, beta = 1, t_zero = 6,num_layers=7 ,include_top = True, num_classes = 10).to(device)\n",
        "model2 = PhiNet.from_pretrained(\"CIFAR-10\", 3.0, 0.75, 6.0, 7, 160, classifier=True).to(device)\n",
        "optimizer2 = Adam(model2.parameters(), lr=lr, weight_decay=0)\n",
        "\n",
        "train_epochs = 10\n",
        "\n",
        "joint_training = JointTraining(\n",
        "    model=model2,\n",
        "    optimizer=optimizer2,\n",
        "    criterion=criterion,\n",
        "    train_mb_size=train_mb_size,\n",
        "    train_epochs=train_epochs,\n",
        "    eval_mb_size=eval_mb_size,\n",
        "    split_ratio = split_ratio,\n",
        "    patience = patience,\n",
        "    device=device,\n",
        "    path='results/models/joint_training.pth'\n",
        ")\n",
        "\n",
        "joint_training.train(train_stream, test_stream, plotting=True)\n",
        "b,c = joint_training.test(test_stream)\n",
        "a = joint_training.get_tasks_acc()\n",
        "accs['Joint Training'] = a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggTVXamGzcW4"
      },
      "source": [
        "### Comulative Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2HgeprNzcW5"
      },
      "outputs": [],
      "source": [
        "#model3 = PhiNet(input_shape = (1, 28, 28), alpha = 0.5, beta = 1, t_zero = 6,num_layers=7 ,include_top = True, num_classes = 10).to(device)\n",
        "model3 = PhiNet.from_pretrained(\"CIFAR-10\", 3.0, 0.75, 6.0, 7, 160, classifier=True).to(device)\n",
        "\n",
        "optimizer3 = Adam(model3.parameters(), lr=lr, weight_decay=0)\n",
        "\n",
        "train_epochs = 10\n",
        "\n",
        "comulative = Comulative(\n",
        "    model=model3,\n",
        "    optimizer=optimizer3,\n",
        "    criterion=criterion,\n",
        "    train_mb_size=train_mb_size,\n",
        "    train_epochs=train_epochs,\n",
        "    eval_mb_size=eval_mb_size,\n",
        "    split_ratio = split_ratio,\n",
        "    patience = patience,\n",
        "    device=device,\n",
        "    path='results/models/Comulative.pt'\n",
        ")\n",
        "\n",
        "comulative.train(train_stream, test_stream, plotting=True)\n",
        "b,c = comulative.test(test_stream)\n",
        "a = comulative.get_tasks_acc()\n",
        "accs['Comulative'] = a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tS1PK2uzcW6"
      },
      "source": [
        "### Replay Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE0_3a8DzcW6"
      },
      "outputs": [],
      "source": [
        "#model4 = PhiNet(input_shape = (1, 28, 28), alpha = 0.5, beta = 1, t_zero = 6,num_layers=7 ,include_top = True, num_classes = 10).to(device)\n",
        "model4 = PhiNet.from_pretrained(\"CIFAR-10\", 3.0, 0.75, 6.0, 7, 160, classifier=True).to(device)\n",
        "\n",
        "optimizer4 = Adam(model4.parameters(), lr=lr, weight_decay=0)\n",
        "\n",
        "storage_p = ReservoirSamplingBuffer(max_size=1500)\n",
        "\n",
        "train_epochs = 10\n",
        "\n",
        "replay = Replay(\n",
        "    model=model4,\n",
        "    optimizer=optimizer4,\n",
        "    criterion=criterion,\n",
        "    train_mb_size=train_mb_size,\n",
        "    train_epochs=train_epochs,\n",
        "    eval_mb_size=eval_mb_size,\n",
        "    storage_policy = storage_p,\n",
        "    split_ratio = split_ratio,\n",
        "    patience = patience,\n",
        "    device=device,\n",
        "    path='results/models/ExpReplay.pth'\n",
        ")\n",
        "\n",
        "replay.train(train_stream, test_stream, plotting=True)\n",
        "b,c = replay.test(test_stream)\n",
        "a = replay.get_tasks_acc()\n",
        "accs['ExpReplay'] = a\n",
        "\n",
        "mac = evaluations.get_MAC(model4, input_size)\n",
        "print(\"MAC: \", mac)\n",
        "\n",
        "mean, std = evaluations.measure_inference_time(model4, input_size)\n",
        "print(f\"Average inference time: {mean:.3f} +/- {std:.3f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ7gVY7H9moR"
      },
      "source": [
        "### Latent Replay Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qextF3Z99moR"
      },
      "outputs": [],
      "source": [
        "model5 = PhiNet.from_pretrained(\"CIFAR-10\", 3.0, 0.75, 6.0, 7, 160, classifier=False).to(device)\n",
        "#model4 = PhiNet(input_shape = (1, 28, 28), alpha = 0.5, beta = 1, t_zero = 6,num_layers=7 ,include_top = False, num_classes = 10).to(device)\n",
        "#model4.load_state_dict(torch.load(\"results/models/7_Layers/Adam.pth\", map_location=torch.device(device)))\n",
        "\n",
        "model5 = PhiNetV3(model5, latent_layer_num = 9, replace_bn_with_brn = True).to(device)\n",
        "optimizer5 = Adam(model5.parameters(), lr=lr, weight_decay=0)\n",
        "\n",
        "train_epochs = 10\n",
        "\n",
        "latent_replay = LatentReplay(\n",
        "    model = model5,\n",
        "    optimizer = optimizer5,\n",
        "    criterion = criterion,\n",
        "    train_mb_size = 21,\n",
        "    replay_mb_size = 107,\n",
        "    train_epochs = train_epochs,\n",
        "    eval_mb_size = eval_mb_size,\n",
        "    rm_size_MB = 3,\n",
        "    manual_mb = True,\n",
        "    split_ratio = split_ratio,\n",
        "    patience = patience,\n",
        "    device = device,\n",
        "    path='results/models/LatentReplay.pth'\n",
        ")\n",
        "\n",
        "latent_replay.train(train_stream, test_stream, plotting=True)\n",
        "b,c = latent_replay.test(test_stream)\n",
        "a = latent_replay.get_tasks_acc()\n",
        "accs['LatentReplay'] = a\n",
        "\n",
        "mac = evaluations.get_MAC(model5, input_size)\n",
        "print(\"MAC: \", mac)\n",
        "\n",
        "mean, std = evaluations.measure_inference_time(model5, input_size)\n",
        "print(f\"Average inference time: {mean:.3f} +/- {std:.3f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the dictionary to a JSON file\n",
        "with open('./results/result.json', 'w') as file:\n",
        "    json.dump(accs, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plotter = utils.TaskAccuracyPlotter()\n",
        "\n",
        "for key, value in accs.items():\n",
        "    _ = plotter.plot_task_accuracy(value, label=key, plot_task_acc=True, plot_avg_acc=True, plot_encountered_avg=True)\n",
        "plotter.show_figures()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
