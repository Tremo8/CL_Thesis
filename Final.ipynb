{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwj4CkTzzcWr"
      },
      "source": [
        "# Latent Replay for Continual Learning on Edge devices with Efficient Architectures\n",
        "In this Jupyter Notebook, we will explore various deep learning strategies for continual learning using the PyTorch framework. We will use the Avalanche library to handle the continual learning benchmarks, and we will train and evaluate different models and strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPXJaZZjzcWu"
      },
      "source": [
        "## Installation\n",
        "Install all the necessary library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzF55R6dzcWv",
        "outputId": "d65bed89-f433-46e7-b780-98c68180e51c"
      },
      "outputs": [],
      "source": [
        "! pip install avalanche-lib==0.3.1\n",
        "! pip install micromind\n",
        "! pip install torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing the necessary libraries and setting up the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E3yHNRPzcWw",
        "outputId": "b9bbc510-11f7-4381-d4dc-5cfb5c67f2a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\matte\\anaconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x28994a29f90>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Standard libraries\n",
        "import json\n",
        "\n",
        "# PyTorch modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import SGD, Adam\n",
        "\n",
        "# Torchvision modules\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Avalanche modules\n",
        "from avalanche.benchmarks.classic import SplitMNIST, SplitCIFAR10\n",
        "from avalanche.models import MobilenetV1\n",
        "from avalanche.training.storage_policy import ReservoirSamplingBuffer\n",
        "\n",
        "# Custom Strategy modules\n",
        "from strategy.joint_training import JointTraining\n",
        "from strategy.fine_tuning import FineTuning\n",
        "from strategy.comulative import Comulative\n",
        "from strategy.replay import Replay\n",
        "from strategy.latent_replay import LatentReplay\n",
        "\n",
        "# Model modules\n",
        "from micromind import PhiNet\n",
        "from model.phinet_v2 import PhiNet_v2\n",
        "from model.phinet_v3 import PhiNetV3\n",
        "\n",
        "# Uutils modules\n",
        "import utility.utils as utils\n",
        "import utility.evaluations as evaluations\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KDShngxzcWy"
      },
      "source": [
        "## Benchmark\n",
        "In this section, we will prepare the data for the continual learning experiments. We will use two classic benchmark datasets: \n",
        "- SplitMNIST;\n",
        "- SplitCIFAR10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MNIST\n",
        "The SplitMNIST dataset with 5 experiences. The SplitMNIST is normailize in Avalnche with `mean = 0.1307` and `std = 0.3081`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i52l8jKzcWy",
        "outputId": "668663c0-52d7-4877-a8d0-d8de1fc5a070"
      },
      "outputs": [],
      "source": [
        "split_mnist = SplitMNIST(n_experiences=5, seed=0, return_task_id = True)\n",
        "\n",
        "# recovering the train and test streams\n",
        "train_stream = split_mnist.train_stream\n",
        "test_stream = split_mnist.test_stream"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CIFAR10\n",
        "The SplitCIFAR10 dataset with 5 experiences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Resize((160, 160)),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "split_cifar = SplitCIFAR10(n_experiences=5, seed=0, return_task_id = True, train_transform = transform, eval_transform = transform)\n",
        "\n",
        "# recovering the train and test streams\n",
        "train_stream = split_cifar.train_stream\n",
        "test_stream = split_cifar.test_stream"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GApUVD8lzcWz"
      },
      "source": [
        "## Training\n",
        "Set up the necessary components for training our deep learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AvfBVXfazcW1"
      },
      "outputs": [],
      "source": [
        "# Loss criterion\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training and evaluation batch size parameters\n",
        "train_mb_size = 32\n",
        "eval_mb_size = 32\n",
        "\n",
        "# Train-Validation split ratio\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Early stopping patience\n",
        "patience = 3\n",
        "\n",
        "# Accuracies dictionary to store accuracies for each strategy\n",
        "accs = dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWJxGNh9zcW2"
      },
      "source": [
        "### Fine Tuning Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "vJFWY3eVzcW2",
        "outputId": "4b52800b-279b-4dfd-bd63-31ec216d6370"
      },
      "outputs": [],
      "source": [
        "#model1 = PhiNet(input_shape = (1, 28, 28), alpha = 0.5, beta = 1, t_zero = 6,num_layers=7 ,include_top = True, num_classes = 10).to(device)\n",
        "model1 = PhiNet.from_pretrained(\"CIFAR-10\", 3.0, 0.75, 6.0, 7, 160, classifier=True).to(device)\n",
        "optimizer1 = Adam(model1.parameters(), lr=0.01, weight_decay=0)\n",
        "\n",
        "train_epochs = 10\n",
        "\n",
        "fine_tuning = FineTuning(\n",
        "    model=model1,\n",
        "    optimizer=optimizer1,\n",
        "    criterion=criterion,\n",
        "    train_mb_size=train_mb_size,\n",
        "    train_epochs=train_epochs,\n",
        "    eval_mb_size=eval_mb_size,\n",
        "    split_ratio = split_ratio,\n",
        "    patience = patience,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "fine_tuning.train(train_stream, test_stream, plotting=True)\n",
        "b,c = fine_tuning.test(test_stream)\n",
        "a = fine_tuning.get_tasks_acc()\n",
        "accs['Fine Tuning'] = a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-tRVDugzcW3"
      },
      "source": [
        "### Joint Training Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsNBEspqzcW4"
      },
      "outputs": [],
      "source": [
        "#model2 = PhiNet(input_shape = (1, 28, 28), alpha = 0.5, beta = 1, t_zero = 6,num_layers=7 ,include_top = True, num_classes = 10).to(device)\n",
        "model2 = PhiNet.from_pretrained(\"CIFAR-10\", 3.0, 0.75, 6.0, 7, 160, classifier=True).to(device)\n",
        "optimizer2 = Adam(model2.parameters(), lr=0.01, weight_decay=0)\n",
        "\n",
        "train_epochs = 10\n",
        "\n",
        "joint_training = JointTraining(\n",
        "    model=model2,\n",
        "    optimizer=optimizer2,\n",
        "    criterion=criterion,\n",
        "    train_mb_size=train_mb_size,\n",
        "    train_epochs=train_epochs,\n",
        "    eval_mb_size=eval_mb_size,\n",
        "    split_ratio = split_ratio,\n",
        "    patience = patience,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "joint_training.train(train_stream, test_stream, plotting=True)\n",
        "b,c = joint_training.test(test_stream)\n",
        "a = joint_training.get_tasks_acc()\n",
        "accs['Joint Training'] = a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggTVXamGzcW4"
      },
      "source": [
        "### Comulative Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2HgeprNzcW5"
      },
      "outputs": [],
      "source": [
        "#model3 = PhiNet(input_shape = (1, 28, 28), alpha = 0.5, beta = 1, t_zero = 6,num_layers=7 ,include_top = True, num_classes = 10).to(device)\n",
        "model3 = PhiNet.from_pretrained(\"CIFAR-10\", 3.0, 0.75, 6.0, 7, 160, classifier=True).to(device)\n",
        "\n",
        "optimizer3 = Adam(model3.parameters(), lr=0.01, weight_decay=0)\n",
        "\n",
        "train_epochs = 10\n",
        "\n",
        "comulative = Comulative(\n",
        "    model=model3,\n",
        "    optimizer=optimizer3,\n",
        "    criterion=criterion,\n",
        "    train_mb_size=train_mb_size,\n",
        "    train_epochs=train_epochs,\n",
        "    eval_mb_size=eval_mb_size,\n",
        "    split_ratio = split_ratio,\n",
        "    patience = patience,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "comulative.train(train_stream, test_stream, plotting=True)\n",
        "b,c = comulative.test(test_stream)\n",
        "a = comulative.get_tasks_acc()\n",
        "accs['Comulative'] = a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tS1PK2uzcW6"
      },
      "source": [
        "### Replay Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE0_3a8DzcW6"
      },
      "outputs": [],
      "source": [
        "#model4 = PhiNet(input_shape = (1, 28, 28), alpha = 0.5, beta = 1, t_zero = 6,num_layers=7 ,include_top = True, num_classes = 10).to(device)\n",
        "model4 = PhiNet.from_pretrained(\"CIFAR-10\", 3.0, 0.75, 6.0, 7, 160, classifier=True).to(device)\n",
        "\n",
        "optimizer4 = Adam(model4.parameters(), lr=0.01, weight_decay=0)\n",
        "\n",
        "storage_p = ReservoirSamplingBuffer(max_size=1500)\n",
        "\n",
        "train_epochs = 10\n",
        "\n",
        "replay = Replay(\n",
        "    model=model4,\n",
        "    optimizer=optimizer4,\n",
        "    criterion=criterion,\n",
        "    train_mb_size=train_mb_size,\n",
        "    train_epochs=train_epochs,\n",
        "    eval_mb_size=eval_mb_size,\n",
        "    storage_policy = storage_p,\n",
        "    split_ratio = split_ratio,\n",
        "    patience = patience,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "replay.train(train_stream, test_stream, plotting=True)\n",
        "b,c = replay.test(test_stream)\n",
        "a = replay.get_tasks_acc()\n",
        "accs['ExpReplay'] = a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ7gVY7H9moR"
      },
      "source": [
        "### Latent Replay Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qextF3Z99moR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start of the training process...\n",
            "Training of the experience with class:  [1, 4]\n",
            "Train dataset size:  10067\n",
            "Validation dataset size:  2517\n",
            "Validation loss decreased (inf --> 0.049161).  Saving model ...\n",
            "Epoch: 1/1, Train Loss: 11.0085, Train Accuracy: 98.41%\n",
            "Early stopping reset.\n",
            "Number of element in the replay memory: 4053\n",
            "-----------------------------------------------------------------------------------\n",
            "Training of the experience with class:  [5, 7]\n",
            "Train dataset size:  9348\n",
            "Validation dataset size:  2338\n",
            "Validation loss decreased (inf --> 0.059193).  Saving model ...\n",
            "Epoch: 1/1, Train Loss: 7.4946, Train Accuracy: 98.55%\n",
            "Early stopping reset.\n",
            "Number of element in the replay memory: 4052\n",
            "-----------------------------------------------------------------------------------\n",
            "Training of the experience with class:  [9, 3]\n",
            "Train dataset size:  9664\n",
            "Validation dataset size:  2416\n",
            "Validation loss decreased (inf --> 0.220718).  Saving model ...\n",
            "Epoch: 1/1, Train Loss: 15.9549, Train Accuracy: 96.84%\n",
            "Early stopping reset.\n",
            "Number of element in the replay memory: 4053\n",
            "-----------------------------------------------------------------------------------\n",
            "Training of the experience with class:  [0, 8]\n",
            "Train dataset size:  9419\n",
            "Validation dataset size:  2355\n",
            "Validation loss decreased (inf --> 0.201844).  Saving model ...\n",
            "Epoch: 1/1, Train Loss: 19.4756, Train Accuracy: 95.99%\n",
            "Early stopping reset.\n",
            "Number of element in the replay memory: 4052\n",
            "-----------------------------------------------------------------------------------\n",
            "Training of the experience with class:  [2, 6]\n",
            "Train dataset size:  9500\n",
            "Validation dataset size:  2376\n",
            "Validation loss decreased (inf --> 0.160465).  Saving model ...\n",
            "Epoch: 1/1, Train Loss: 18.8841, Train Accuracy: 96.53%\n",
            "Early stopping reset.\n",
            "Number of element in the replay memory: 4050\n",
            "-----------------------------------------------------------------------------------\n",
            "Size of the replay memory: 3.00 MB\n",
            "End of the training process.\n",
            "\n",
            "MAC: 1046686\n",
            "Average inference time: 16.729 +/- 5.196 ms\n"
          ]
        }
      ],
      "source": [
        "#model4 = PhiNet.from_pretrained(\"CIFAR-10\", 3.0, 0.75, 6.0, 7, 160, classifier=False).to(device)\n",
        "model4 = PhiNet(input_shape = (1, 28, 28), alpha = 0.5, beta = 1, t_zero = 6,num_layers=7 ,include_top = False, num_classes = 10).to(device)\n",
        "model4.load_state_dict(torch.load(\"TestModel/7_Layers/Adam.pth\", map_location=torch.device(device)))\n",
        "\n",
        "model4 = PhiNetV3(model4, latent_layer_num = 9, replace_bn_with_brn = True).to(device)\n",
        "optimizer4 = Adam(model4.parameters(), lr=0.01, weight_decay=0)\n",
        "\n",
        "train_epochs = 1\n",
        "\n",
        "latent_replay = LatentReplay(\n",
        "    model = model4,\n",
        "    optimizer = optimizer4,\n",
        "    criterion = criterion,\n",
        "    train_mb_size = 21,\n",
        "    replay_mb_size = 107,\n",
        "    train_epochs = train_epochs,\n",
        "    eval_mb_size = eval_mb_size,\n",
        "    rm_size_MB = 3,\n",
        "    manual_mb = True,\n",
        "    split_ratio = split_ratio,\n",
        "    patience = patience,\n",
        "    device = device\n",
        ")\n",
        "\n",
        "latent_replay.train(train_stream, plotting=False)\n",
        "#b,c = latent_replay.test(test_stream)\n",
        "a = latent_replay.get_tasks_acc()\n",
        "accs['LatentReplay'] = a\n",
        "\n",
        "mac = evaluations.get_MAC(model4, (1,28,28))\n",
        "print(f\"MAC: {mac}\")\n",
        "\n",
        "mean, std = evaluations.measure_inference_time(model4, (1,28,28))\n",
        "print(f\"Average inference time: {mean:.3f} +/- {std:.3f} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the dictionary to a JSON file\n",
        "with open('./results/result.json', 'w') as file:\n",
        "    json.dump(accs, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('./results/result.json', 'r') as file:\n",
        "    accs = json.load(file)\n",
        "\n",
        "plotter = utils.TaskAccuracyPlotter()\n",
        "\n",
        "for key, value in accs.items():\n",
        "    _ = plotter.plot_task_accuracy(value, label=key, plot_task_acc=True, plot_avg_acc=True, plot_encountered_avg=True)\n",
        "plotter.show_figures()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
