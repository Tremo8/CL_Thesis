{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Notebook\n",
    "Load a trained model to show the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.phinet_v3 import PhiNetV3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from micromind import PhiNet\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Avalanche modules\n",
    "from avalanche.benchmarks.classic import SplitCIFAR10\n",
    "\n",
    "import utility.utils as utils\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def test(dataset, model, criterion, device, eval_mb_size):       \n",
    "        sum_accuracy = 0\n",
    "        exps_acc = dict()\n",
    "        results = [[],[]]\n",
    "        for exp in dataset:\n",
    "            print(\"Testing task \", exp.task_label)\n",
    "            print('Classes in this task:', exp.classes_in_this_experience)\n",
    "\n",
    "            experience_dataloader = DataLoader(exp.dataset, batch_size=eval_mb_size, shuffle=False)\n",
    "            test_acc, test_loss = utils.test(model, criterion, experience_dataloader, device)\n",
    "            sum_accuracy += test_acc\n",
    "            print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n",
    "            exps_acc[exp.task_label] = test_acc\n",
    "\n",
    "            results[0].append(f\"Task {exp.task_label}\")\n",
    "            results[1].append(test_acc)\n",
    "\n",
    "        # Calculate and add average accuracy\n",
    "        avg_accuracy = sum_accuracy / len(dataset)\n",
    "        results[0].append(f\"Avg Acc\")\n",
    "        results[1].append(avg_accuracy)\n",
    "\n",
    "        print(f\"Average accuracy: {avg_accuracy:.2f}%\")\n",
    "\n",
    "# Define a function to visualize predictions for a specific task\n",
    "def visualize_predictions_for_task(model, dataset, device, num_images=5):\n",
    "    data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=16,\n",
    "            shuffle=True,\n",
    "        )\n",
    "    classes = dataset.targets\n",
    "    classes = ('plane', 'car', 'bird', 'cat',\n",
    "               'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs, targets, _ = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            for i in range(len(targets)):\n",
    "                image = np.transpose(inputs[i].cpu().numpy(), (1, 2, 0))\n",
    "                true_label = classes[targets[i]]\n",
    "\n",
    "                predicted_label = classes[predicted[i]]\n",
    "\n",
    "                plt.imshow(image)\n",
    "                plt.title(f'True: {true_label}, Predicted: {predicted_label}')\n",
    "                plt.show()\n",
    "\n",
    "                count += 1\n",
    "                if count >= num_images:\n",
    "                    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "split_cifar = SplitCIFAR10(n_experiences=5, seed=0, return_task_id = True, train_transform = transform, eval_transform = transform)\n",
    "\n",
    "# recovering the train and test streams\n",
    "train_stream = split_cifar.train_stream\n",
    "test_stream = split_cifar.test_stream\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# Loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "input_shape = (3, 224, 224)\n",
    "torch.cuda.set_device(2) if torch.cuda.is_available() else None\n",
    "\n",
    "# Set the device as cuda, the GPU specified as default will be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(f\"Device: {device}\")    \n",
    "\n",
    "phinet = PhiNet(input_shape = input_shape, alpha = 3, beta = 0.75, t_zero = 6, num_layers=7 ,include_top = False, num_classes = 1000).to(device)\n",
    "\n",
    "model = PhiNetV3(phinet, latent_layer_num = 10).to(device)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "saved_state_dict = torch.load(\"./results/latent_10/weight_decay_0/lr_5e-05_epochs_10_rm_MB_325_rm_None_split_0.8.pth\", map_location=torch.device(device))  # Replace with the actual path\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(saved_state_dict)\n",
    "\n",
    "test(test_stream, model, criterion, device, eval_mb_size=16)\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "for exp in test_stream:\n",
    "    visualize_predictions_for_task(model, exp.dataset, device, num_images=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
