{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Notebook\n",
    "Load a trained model to show the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matte\\anaconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model.phinet_v3 import PhiNetV3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from micromind import PhiNet\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Avalanche modules\n",
    "from avalanche.benchmarks.classic import SplitCIFAR10\n",
    "\n",
    "import utility.utils as utils\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataset, model, criterion, device, eval_mb_size):       \n",
    "        sum_accuracy = 0\n",
    "        exps_acc = dict()\n",
    "        results = [[],[]]\n",
    "        for exp in dataset:\n",
    "            print(\"Testing task \", exp.task_label)\n",
    "            print('Classes in this task:', exp.classes_in_this_experience)\n",
    "\n",
    "            experience_dataloader = DataLoader(exp.dataset, batch_size=eval_mb_size, shuffle=False)\n",
    "            test_acc, test_loss = utils.test(model, criterion, experience_dataloader, device)\n",
    "            sum_accuracy += test_acc\n",
    "            print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n",
    "            exps_acc[exp.task_label] = test_acc\n",
    "\n",
    "            results[0].append(f\"Task {exp.task_label}\")\n",
    "            results[1].append(test_acc)\n",
    "\n",
    "        # Calculate and add average accuracy\n",
    "        avg_accuracy = sum_accuracy / len(dataset)\n",
    "        results[0].append(f\"Avg Acc\")\n",
    "        results[1].append(avg_accuracy)\n",
    "\n",
    "        print(f\"Average accuracy: {avg_accuracy:.2f}%\")\n",
    "\n",
    "# Define a function to visualize predictions for a specific task\n",
    "def visualize_predictions_for_task(model, dataset, device, num_images=5):\n",
    "    data_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=16,\n",
    "            shuffle=True,\n",
    "        )\n",
    "    classes = dataset.targets\n",
    "    classes = ('plane', 'car', 'bird', 'cat',\n",
    "               'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs, targets, _ = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            for i in range(len(targets)):\n",
    "                image = np.transpose(inputs[i].cpu().numpy(), (1, 2, 0))\n",
    "                true_label = classes[targets[i]]\n",
    "\n",
    "                predicted_label = classes[predicted[i]]\n",
    "\n",
    "                plt.imshow(image)\n",
    "                plt.title(f'True: {true_label}, Predicted: {predicted_label}')\n",
    "                plt.show()\n",
    "\n",
    "                count += 1\n",
    "                if count >= num_images:\n",
    "                    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "split_cifar = SplitCIFAR10(n_experiences=5, seed=0, return_task_id = True, train_transform = transform, eval_transform = transform)\n",
    "\n",
    "# recovering the train and test streams\n",
    "train_stream = split_cifar.train_stream\n",
    "test_stream = split_cifar.test_stream\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# Loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "input_shape = (3, 224, 224)\n",
    "torch.cuda.set_device(2) if torch.cuda.is_available() else None\n",
    "\n",
    "# Set the device as cuda, the GPU specified as default will be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(f\"Device: {device}\")    \n",
    "\n",
    "phinet = PhiNet(input_shape = input_shape, alpha = 3, beta = 0.75, t_zero = 6, num_layers=7 ,include_top = False, num_classes = 1000).to(device)\n",
    "\n",
    "model = PhiNetV3(phinet, latent_layer_num = 10).to(device)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "saved_state_dict = torch.load(\"./results/latent_10/weight_decay_0/lr_5e-05_epochs_10_rm_MB_325_rm_None_split_0.8.pth\", map_location=torch.device(device))  # Replace with the actual path\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(saved_state_dict)\n",
    "\n",
    "test(test_stream, model, criterion, device, eval_mb_size=16)\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "for exp in test_stream:\n",
    "    visualize_predictions_for_task(model, exp.dataset, device, num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Checkpoint taken from local file system.\n",
      "Checkpoint loaded successfully.\n",
      "PhiNet(\n",
      "  (_layers): ModuleList(\n",
      "    (0): ZeroPad2d((0, 1, 0, 1))\n",
      "    (1): SeparableConv2d(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(2, 2), groups=3, bias=False)\n",
      "        (1): Conv2d(3, 110, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(110, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "      )\n",
      "    )\n",
      "    (2): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Dropout2d(p=0.05, inplace=False)\n",
      "        (1): DepthwiseConv2d(110, 110, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=110, bias=False)\n",
      "        (2): BatchNorm2d(110, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (3): Hardswish()\n",
      "        (4): Conv2d(110, 55, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(55, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(55, 265, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(265, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(265, 265, kernel_size=(3, 3), stride=(2, 2), groups=265, bias=False)\n",
      "        (6): BatchNorm2d(265, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): Hardswish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(265, 44, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(44, 265, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "        )\n",
      "        (9): Conv2d(265, 55, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(55, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(55, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(255, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(255, 255, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=255, bias=False)\n",
      "        (5): BatchNorm2d(255, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(255, 42, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(42, 255, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "        )\n",
      "        (8): Conv2d(255, 55, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(55, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(55, 245, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(245, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(245, 245, kernel_size=(3, 3), stride=(2, 2), groups=245, bias=False)\n",
      "        (6): BatchNorm2d(245, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): Hardswish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(245, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(40, 245, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "        )\n",
      "        (9): Conv2d(245, 110, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(110, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(110, 471, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(471, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(471, 471, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=471, bias=False)\n",
      "        (5): BatchNorm2d(471, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(471, 78, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(78, 471, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "        )\n",
      "        (8): Conv2d(471, 110, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(110, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(110, 451, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(451, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(451, 451, kernel_size=(3, 3), stride=(2, 2), groups=451, bias=False)\n",
      "        (6): BatchNorm2d(451, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): Hardswish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(451, 75, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(75, 451, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "        )\n",
      "        (9): Conv2d(451, 220, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(220, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(220, 864, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(864, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): Dropout2d(p=0.05, inplace=False)\n",
      "        (4): DepthwiseConv2d(864, 864, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=864, bias=False)\n",
      "        (5): BatchNorm2d(864, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (6): Hardswish()\n",
      "        (7): SEBlock(\n",
      "          (se_conv): Conv2d(864, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(144, 864, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "        )\n",
      "        (8): Conv2d(864, 220, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(220, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): PhiNetConvBlock(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Conv2d(220, 825, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(825, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "        (3): ZeroPad2d((1, 1, 1, 1))\n",
      "        (4): Dropout2d(p=0.05, inplace=False)\n",
      "        (5): DepthwiseConv2d(825, 825, kernel_size=(3, 3), stride=(2, 2), groups=825, bias=False)\n",
      "        (6): BatchNorm2d(825, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "        (7): Hardswish()\n",
      "        (8): SEBlock(\n",
      "          (se_conv): Conv2d(825, 137, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (se_conv2): Conv2d(137, 825, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (activation): Hardswish()\n",
      "        )\n",
      "        (9): Conv2d(825, 441, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (10): BatchNorm2d(441, eps=0.001, momentum=0.999, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(f\"Device: {device}\")  \n",
    "\n",
    "model = PhiNet.from_pretrained(\"ImageNet-1k\",2.0, 0.75, 6.0, 7, 224, num_classes=1000, path=\"phinet2.pth.tar\", classifier=False, device = device)\n",
    "\n",
    "print(model)\n",
    "#summary(model, input_size=(1, 3, 224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "PhiNetV3                                      [1, 10]                   --\n",
       "├─Sequential: 1-1                             [1, 441, 7, 7]            --\n",
       "│    └─ZeroPad2d: 2-1                         [1, 3, 225, 225]          --\n",
       "│    └─SeparableConv2d: 2-2                   [1, 110, 112, 112]        --\n",
       "│    │    └─ModuleList: 3-1                   --                        577\n",
       "│    └─PhiNetConvBlock: 2-3                   [1, 55, 112, 112]         --\n",
       "│    │    └─ModuleList: 3-2                   --                        7,370\n",
       "│    └─PhiNetConvBlock: 2-4                   [1, 55, 56, 56]           --\n",
       "│    │    └─ModuleList: 3-3                   --                        56,025\n",
       "│    └─PhiNetConvBlock: 2-5                   [1, 55, 56, 56]           --\n",
       "│    │    └─ModuleList: 3-4                   --                        52,895\n",
       "│    └─PhiNetConvBlock: 2-6                   [1, 110, 28, 28]          --\n",
       "│    │    └─ModuleList: 3-5                   --                        63,430\n",
       "│    └─PhiNetConvBlock: 2-7                   [1, 110, 28, 28]          --\n",
       "│    │    └─ModuleList: 3-6                   --                        183,439\n",
       "│    └─PhiNetConvBlock: 2-8                   [1, 220, 14, 14]          --\n",
       "│    │    └─ModuleList: 3-7                   --                        222,783\n",
       "│    └─PhiNetConvBlock: 2-9                   [1, 220, 14, 14]          --\n",
       "│    │    └─ModuleList: 3-8                   --                        640,664\n",
       "│    └─PhiNetConvBlock: 2-10                  [1, 441, 7, 7]            --\n",
       "│    │    └─ModuleList: 3-9                   --                        782,982\n",
       "├─Sequential: 1-2                             [1, 441, 7, 7]            --\n",
       "├─Sequential: 1-3                             [1, 10]                   --\n",
       "│    └─AdaptiveAvgPool2d: 2-11                [1, 441, 1, 1]            --\n",
       "│    └─Flatten: 2-12                          [1, 441]                  --\n",
       "│    └─Linear: 2-13                           [1, 10]                   4,420\n",
       "===============================================================================================\n",
       "Total params: 2,014,585\n",
       "Trainable params: 2,014,585\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 763.28\n",
       "===============================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 200.51\n",
       "Params size (MB): 8.06\n",
       "Estimated Total Size (MB): 209.17\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = PhiNetV3(model, latent_layer_num = 10).to(device)\n",
    "\n",
    "summary(model2, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[0.5145, 0.0246, 0.7773],\n",
      "        [0.2114, 0.8546, 0.9567],\n",
      "        [0.1866, 0.3325, 0.4517],\n",
      "        [0.7924, 0.1082, 0.0326],\n",
      "        [0.6336, 0.4259, 0.4195]])\n",
      "x.dtype: torch.float32\n",
      "Dim in B: 60\n",
      "x: tensor([[1., 0., 1.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.]], size=(5, 3), dtype=torch.quint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=1.0, zero_point=0)\n",
      "x.dtype: torch.quint8\n",
      "Dim in B: 15\n",
      "x: tensor([[1., 0., 1.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.]])\n",
      "x.dtype: torch.float32\n",
      "Dim in B: 60\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(5,3, dtype=torch.float32)\n",
    "\n",
    "print(f\"x: {x}\")   \n",
    "print(f\"x.dtype: {x.dtype}\")\n",
    "print(f\"Dim in B: {x.element_size() * x.nelement()}\")\n",
    "\n",
    "\n",
    "x = torch.quantize_per_tensor(x, 1.0, 0, torch.quint8)\n",
    "\n",
    "print(f\"x: {x}\")   \n",
    "print(f\"x.dtype: {x.dtype}\")\n",
    "print(f\"Dim in B: {x.element_size() * x.nelement()}\")\n",
    "\n",
    "x = torch.dequantize(x)\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"x.dtype: {x.dtype}\")\n",
    "print(f\"Dim in B: {x.element_size() * x.nelement()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
