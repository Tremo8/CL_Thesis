{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Notebook\n",
    "Load a trained model to show the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.phinet_v3 import PhiNetV3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from micromind import PhiNet\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Avalanche modules\n",
    "from avalanche.benchmarks.classic import SplitCIFAR10\n",
    "\n",
    "import utility.utils as utils\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def test(dataset, model, criterion, device, eval_mb_size):       \n",
    "        sum_accuracy = 0\n",
    "        exps_acc = dict()\n",
    "        results = [[],[]]\n",
    "        for exp in dataset:\n",
    "            print(\"Testing task \", exp.task_label)\n",
    "            print('Classes in this task:', exp.classes_in_this_experience)\n",
    "\n",
    "            experience_dataloader = DataLoader(exp.dataset, batch_size=eval_mb_size, shuffle=False)\n",
    "            test_acc, test_loss = utils.test(model, criterion, experience_dataloader, device)\n",
    "            sum_accuracy += test_acc\n",
    "            print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n",
    "            exps_acc[exp.task_label] = test_acc\n",
    "\n",
    "            results[0].append(f\"Task {exp.task_label}\")\n",
    "            results[1].append(test_acc)\n",
    "\n",
    "        # Calculate and add average accuracy\n",
    "        avg_accuracy = sum_accuracy / len(dataset)\n",
    "        results[0].append(f\"Avg Acc\")\n",
    "        results[1].append(avg_accuracy)\n",
    "\n",
    "        print(f\"Average accuracy: {avg_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "split_cifar = SplitCIFAR10(n_experiences=5, seed=0, return_task_id = True, train_transform = transform, eval_transform = transform)\n",
    "\n",
    "# recovering the train and test streams\n",
    "train_stream = split_cifar.train_stream\n",
    "test_stream = split_cifar.test_stream\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# Loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "input_shape = (3, 224, 224)\n",
    "torch.cuda.set_device(1) if torch.cuda.is_available() else None\n",
    "\n",
    "# Set the device as cuda, the GPU specified as default will be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(f\"Device: {device}\")    \n",
    "\n",
    "phinet = PhiNet(input_shape = input_shape, alpha = 3, beta = 0.75, t_zero = 6, num_layers=7 ,include_top = False, num_classes = 1000).to(device)\n",
    "\n",
    "model = PhiNetV3(phinet, latent_layer_num = 9).to(device)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "saved_state_dict = torch.load(\"./results/latent_9/lr_0.001_epochs_2_rm_MB_None_rm_1500_split_0.0.pth\", map_location=torch.device(device))  # Replace with the actual path\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(saved_state_dict)\n",
    "\n",
    "test(test_stream, model, criterion, device, eval_mb_size=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
